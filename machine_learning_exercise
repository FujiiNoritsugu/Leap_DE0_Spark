1.Python
1.1 数字のリストを作る
1.2 キーが文字列、値が数値のディクショナリを作る
1.3 1.1で作ったリストから3番目から5番目の要素でリストを作る
1.4 1.3で作ったリストの内、10以上の数値を出力する
1.5 パラメータにディクショナリとキーを取り、ディクショナリからキーに対応する値を表示する関数を作る
1.6 コンストラクタにリストを取り、パラメータのインデックス以降のリストを出力するメソッドを持つクラスを作る
1.7 1.6で作ったクラスのコンストラクタにNumpyの配列を設定できるようにする、
またコンストラクタのNumpy配列にパラメータのNumpy配列を乗算するメソッドを追加する
1.8 1.7のクラスに、コンストラクタのNumpy配列からパラメータの数値よりも大きい要素のみの配列を抽出するメソッドを追加する
1.9 1.8のクラスがインスタンス化されている前提で、パラメータの数値に数値をとり、これをパラメータにして1.8で作成したメソッドを呼び出す関数を作成する。
（lambda記法を使ってもよい）
1.10 1.9で作成した関数をパラメータとし、その結果を返却する関数を作成する。（関数を引数にとる関数）
1.11 1.9で作成した関数の実行結果をプロットする

2.パーセプトロン
2.1 AND,NAND,ORゲートを使ってXORゲートを作成する

3.ニューラルネットワーク
3.1 活性化関数とは何かを説明する文を書く
3.2 ステップ関数をPythonで実装し(p45)、プロットする(p47)
3.3 シグモイド関数をPythonで実装し(p48)、プロットする(p49)
3.4 Numpyでサイズ2のリストと2行×3列の行列の内積を計算する(p57)
3.5 パラメータをリストとし、重み(W)との内積とバイアス(b)との和を3.3のシグモイド関数に適用させるメソッドを作成する
3.6 p65の「init_network」をコンストラクタとし「forward」をメソッドとするクラスを作成する
3.7 ソフトマックス関数はどこで使用されるのか、またその理由を説明する文を書く
3.8 ソフトマックス関数をPythonで実装する(p69, p70)
3.9 MINISTデータセットを使用してニューラルネットワークの推論の計算を行う(p76〜p79)
できれば3.6で作成したクラスを修正して計算する
3.10 ニューラルネットワークの計算でのバッチとは何か説明する文を書く、
またバッチ処理を行う利点を書く
3.11 3.9の計算式をバッチ処理対応に変更する(p80〜p81)

4.ニューラルネットワークの学習
4.1 損失関数とは何かを説明する文を書く
4.2 ２乗和誤差関数をPythonで実装する(p89)
4.3 交差エントロピー誤差関数をPythonで実装する(p91)
4.4 ミニバッチ学習はどのように学習を行うのかを説明する文を書く
4.5 4.3の交差エントロピー誤差関数をミニバッチ学習用に変更する(p94)
4.6 勾配法（勾配降下法）とは何かを説明する文を書く
4.7 勾配法を行う関数をPythonで実装する(p107〜p108)、その際数値微分を行う関数も実装する(p104)
4.8 いままでの機能をまとめて学習アルゴリズム（ミニバッチ→勾配の計算→パラメータの更新）として実装する(p114)
4.9 4.8で実装した学習アルゴリズムをミニバッチ学習用に変更する。(p118)
4.10 4.9で変更した学習アルゴリズムを使用してテストデータを評価し、結果をプロットする(p120〜p121)
※テストデータの評価に時間がかかるようであれば、numerical_gradientでなくgradientを使用する

5.誤差逆伝搬法
5.1 計算グラフとは何かを説明する文を書き、簡単な計算（足し算）を計算グラフを描く
5.2 計算グラフの利点を説明する文を描く
5.3 計算グラフの逆伝搬を説明する文を書き、簡単な計算グラフの逆伝搬を記入する(p131〜P132)
